

https://www.jianshu.com/p/e817b2bcab63
https://www.leiphone.com/news/201703/3qMp45aQtbxTdzmK.html
http://cs231n.github.io/neural-networks-2/#init
## 为什么不能全部初始化为0或者相同值

权值0初始化：假如权值全部初始化为0，那么给网络输出数据后经过和权值的计算，前向传播计算的每个隐层的输出都是一样的，隐层的激活函数值是完全一样，而在计算反向传播的时候对于隐层的梯度仍然是一样的，也就说隐层中的神经元是镜像**对称的**，这样在多次迭代中隐层的神经元仍然是这样镜像对称的，都在计算这完全一样的函数，而权值的更新也就是完全一样的，总结起来就是，权值初始化为0，隐层单元都进行相同的计算，两个隐层单元对输出单元的影响也一样的大，经过迭代后同样的对称性依然存在，在经过相同的更新，不管多少次的迭代，隐层单元任然计算完全一样的函数。在这种情况下，多个隐层就没有真的意义，因为他们都计算这相同的事情

## 随机初始化
https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit
初始值过大过小导致梯度传递慢

### 随机初始化很大值
对于sigmoid、tanh激活函数，初始化值太大，使得流入激活函数的值过大，造成饱和现象，当反相传播时，会使得梯度极小，导致梯度弥散。（ReLU函数不受此影响）
### 随机初始化很小值
初始化太小时，经过多层网络，输出值变得极小，在反向传播时也会使得梯度极小，导致梯度弥散。（ReLU函数也受此影响！）

## Xavier
https://zhuanlan.zhihu.com/p/27919794
https://zhuanlan.zhihu.com/p/25110150

## He 初始化 配合Relu
He initialization的思想是：在ReLU网络中，假定每一层有一半的神经元被激活，另一半为0，所以，要保持variance不变，只需要在Xavier的基础上再除以2：

