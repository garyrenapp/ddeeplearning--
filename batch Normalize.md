https://zhuanlan.zhihu.com/p/34879333
## 背景
**什么是Internal Covariate Shift**
一方面，当底层网络中参数发生微弱变化时，由于每一层中的线性变换与非线性激活映射，这些微弱变化随着网络层数的加深而被放大（类似蝴蝶效应）；另一方面，参数的变化导致每一层的输入分布会发生改变，进而上层的网络需要不停地去适应这些分布变化，使得我们的模型训练变得困难。上述这一现象叫做Internal Covariate Shift。

**internal Covariate Shift会带来什么问题？**
（1）上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低
我们在上面提到了梯度下降的过程会让每一层的参数 $W^{[l]} 和 b^{[l]} $发生变化，进而使得每一层的线性与非线性计算结果分布产生变化。后层网络就要不停地去适应这种分布变化，这个时候就会使得整个网络的学习速率过慢。
（2）网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度
当我们在神经网络中采用饱和激活函数（saturated activation function）时，例如sigmoid，tanh激活函数，很容易使得模型训练陷入梯度饱和区（saturated regime）。随着模型训练的进行，我们的参数$ W^{[l]} $会逐渐更新并变大，此时$ Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$ 就会随之变大，并且 $Z^{[l]} $还受到更底层网络参数 $W^{[1]},W^{[2]},\cdots,W^{[l-1]}$ 的影响，随着网络层数的加深， $Z^{[l]} $很容易陷入梯度饱和区，此时梯度会变得很小甚至接近于0，参数的更新速度就会减慢，进而就会放慢网络的收敛速度。

对于激活函数梯度饱和问题，有两种解决思路。第一种就是更为非饱和性激活函数，例如线性整流函数ReLU可以在一定程度上解决训练进入梯度饱和区的问题。另一种思路是，我们可以让激活函数的输入分布保持在一个稳定状态来尽可能避免它们陷入梯度饱和区，这也就是Normalization的思路。
**我们如何减缓Internal Covariate Shift？**
要缓解ICS的问题，就要明白它产生的原因。ICS产生的原因是由于参数更新带来的网络中每一层输入值分布的改变，并且随着网络层数的加深而变得更加严重，因此我们可以通过固定每一层网络输入值的分布来对减缓ICS问题。

（1）白化（Whitening）

白化（Whitening）是机器学习里面常用的一种规范化数据分布的方法，主要是PCA白化与ZCA白化。白化是对输入数据分布进行变换，进而达到以下两个目的：

使得输入特征分布具有相同的均值与方差。其中PCA白化保证了所有特征分布均值为0，方差为1；而ZCA白化则保证了所有特征分布均值为0，方差相同；
去除特征之间的相关性。
通过白化操作，我们可以减缓ICS的问题，进而固定了每一层网络输入分布，加速网络训练过程的收敛（LeCun et al.,1998b；Wiesler&Ney,2011）。

（2）Batch Normalization提出

既然白化可以解决这个问题，为什么我们还要提出别的解决办法？当然是现有的方法具有一定的缺陷，白化主要有以下两个问题：

白化过程计算成本太高，并且在每一轮训练中的每一层我们都需要做如此高成本计算的白化操作；
白化过程由于改变了网络每一层的分布，因而改变了网络层中本身数据的表达能力。底层网络学习到的参数信息会被白化操作丢失掉。
既然有了上面两个问题，那我们的解决思路就很简单，一方面，我们提出的normalization方法要能够简化计算过程；另一方面又需要经过规范化处理后让数据尽可能保留原始的表达能力。于是就有了简化+改进版的白化——Batch Normalization。

## 优势
（1）BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度

BN通过规范化与线性变换使得每一层网络的输入数据的均值与方差都在一定范围内，使得后一层网络不必不断去适应底层网络中输入的变化，从而实现了网络中层与层之间的解耦，允许每一层进行独立学习，有利于提高整个神经网络的学习速度。

（2）BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定
（3）BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题

在不使用BN层的时候，由于网络的深度与复杂性，很容易使得底层网络变化累积到上层网络中，导致模型的训练很容易进入到激活函数的梯度饱和区；通过normalize操作可以让激活函数的输入数据落在梯度非饱和区，缓解梯度消失的问题；另外通过自适应学习 $\gamma 与 \beta $又让数据保留更多的原始信息。

（4）BN具有一定的正则化效果

在Batch Normalization中，由于我们使用mini-batch的均值与方差作为对整体训练样本均值与方差的估计，尽管每一个batch中的数据都是从总体样本中抽样得到，但不同mini-batch的均值与方差会有所不同，这就为网络的学习过程中增加了随机噪音，与Dropout通过关闭神经元给网络训练带来噪音类似，在一定程度上对模型起到了正则化的效果。

## 公式
$$均值 \mu = \frac{1}{m}\sum_{i}{z^{(i)}}$$
$$方差 \sigma^{2} = \frac{1}{m}\sum_{i}{(z^{(i)}-\mu)^{2}} $$
$$z_{norm}^{(i)} = \frac{z^{(i)}-\mu}{\sqrt{\sigma^{2}+\epsilon}} $$ 
$$\hat{z}^{(i)} = \gamma z_{norm}^{(i)}  + \beta $$
## $ \gamma 和 \beta 是可训练参数$
如同上面提到的，Normalization操作我们虽然缓解了ICS问题，让每一层网络的输入数据分布都变得稳定，但却导致了数据表达能力的缺失。也就是我们通过变换操作改变了原有数据的信息表达（representation ability of the network），使得底层网络学习到的参数信息丢失。另一方面，通过让每一层的输入分布均值为0，方差为1，会使得输入在经过sigmoid或tanh激活函数时，容易陷入非线性激活函数的线性区域。

因此，BN又引入了两个可学习（learnable）的参数$ \gamma 与 \beta$ 。这两个参数的引入是为了恢复数据本身的表达能力，对规范化后的数据进行线性变换，即 $\tilde{Z_j}=\gamma_j \hat{Z}_j+\beta_j 。特别地，当 \gamma^2=\sigma^2,\beta=\mu 时$，可以实现等价变换（identity transform）并且保留了原始输入特征的分布信息。
## 训练时代码
```python
    sample_mean = np.mean(x, axis = 0)
    sample_var = np.var(x , axis = 0)
    x_hat = (x - sample_mean) / (np.sqrt(sample_var  + eps))
    out = gamma * x_hat + beta
    cache = (gamma, x, sample_mean, sample_var, eps, x_hat)
    #runing_mean running_var 使用加权平移 也可以采用指数加权平移 用于测试
    running_mean = momentum * running_mean + (1 - momentum) * sample_mean
    running_var = momentum * running_var + (1 - momentum) * sample_var
```
## 测试时代码
```python
    scale = gamma / (np.sqrt(running_var  + eps))
    out = (x - running_mean) * scale + beta
```

## 反向传播


## 补充知识

1 什么是归一化/标准化Normalization是一个统计学中的概念，我们可以叫它归一化或者规范化，它并不是一个完全定义好的数学操作(如加减乘除)。它通过将数据进行偏移和尺度缩放调整，在数据预处理时是非常常见的操作，在网络的中间层如今也很频繁的被使用。1. 线性归一化最简单来说，归一化是指将数据约束到固定的分布范围，比如8位图像的0～255像素值，比如0～1。在数字图像处理领域有一个很常见的线性对比度拉伸操作：X=(x-xmin)/(xmax-mxin)它常常可以实现下面的增强对比度的效果。<img src="https://pic3.zhimg.com/50/v2-6fcffc5dc883e4f6295907f3273bcb81_hd.jpg" data-caption="" data-size="normal" data-rawwidth="1024" data-rawheight="765" class="origin_image zh-lightbox-thumb" width="1024" data-original="https://pic3.zhimg.com/v2-6fcffc5dc883e4f6295907f3273bcb81_r.jpg"/>不过以上的归一化方法有个非常致命的缺陷，当X最大值或者最小值为孤立的极值点，会影响性能。
2. 零均值归一化/Z-score标准化零均值归一化也是一个常见的归一化方法，被称为标准化方法，即每一变量值与其平均值之差除以该变量的标准差。<img src="https://pic4.zhimg.com/50/v2-4c89956d0de6c023390964ae2196e408_hd.jpg" data-caption="" data-size="normal" data-rawwidth="238" data-rawheight="120" class="content_image" width="238"/>经过处理后的数据符合均值为0，标准差为1的分布，如果原始的分布是正态分布，那么z-score标准化就将原始的正态分布转换为标准正态分布，机器学习中的很多问题都是基于正态分布的假设，这是更加常用的归一化方法。
   
*自己的理解*：之前bn我一直理解的是归一化到[0-1]其实不是的，它只是改变了数据的分布，使得数据的均值为0,方差为1
```python
>>> import numpy as np
>>> a = np.array([1,2,1000,4])
>>> a.mean()
251.75
>>> a.std()
432.0036892203584
>>> b =( a - a.mean() ) / a.std()
>>> b
array([-0.58043486, -0.57812006,  1.73204539, -0.57349047])
>>> b.mean()
2.7755575615628914e-17
>>> b.std()
1.0
```