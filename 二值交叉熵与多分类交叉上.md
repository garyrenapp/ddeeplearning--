这个总结的不错 

https://blog.csdn.net/m0_37393514/article/details/81393819

https://zhuanlan.zhihu.com/p/48078990

## BCE binary cross entropy
$ costJ = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)}) $
### 代码
```python
import tensorflow as tf
import numpy as np

def sigmoid(x):
    return 1.0/(1+np.exp(-x))

y = np.array([[1,0,0],[0,1,0],[0,0,1],[1,1,0],[0,1,0]])
logits = np.array([[12,3,2],[3,10,1],[1,2,5],[4,6.5,1.2],[3,6,1]])
y_pred = sigmoid(logits)

BCE1 = -y*np.log(y_pred)-(1-y)*np.log(1-y_pred)
print(BCE1)
[[6.14419348e-06 3.04858735e+00 2.12692801e+00]
 [3.04858735e+00 4.53988992e-05 1.31326169e+00]
 [1.31326169e+00 2.12692801e+00 6.71534849e-03]
 [1.81499279e-02 1.50231016e-03 1.46328247e+00]
 [3.04858735e+00 2.47568514e-03 1.31326169e+00]]

sess =tf.Session()
y = np.array(y).astype(np.float64)
BCE2 = sess.run(tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=logits))

print(BCE2)
[[6.14419348e-06 3.04858735e+00 2.12692801e+00]
 [3.04858735e+00 4.53988992e-05 1.31326169e+00]
 [1.31326169e+00 2.12692801e+00 6.71534849e-03]
 [1.81499279e-02 1.50231016e-03 1.46328247e+00]
 [3.04858735e+00 2.47568514e-03 1.31326169e+00]]
```
## categorical cross entropy
$ costJ = -\frac{1}{m}\sum_{i=1}^{m}-y^{(i)}\log(a^{(i)})) $
### 代码
```python
def softmax(x):
    sum_raw = np.sum(np.exp(x),axis=-1)
    x1 = np.ones(np.shape(x))
    for i in range(np.shape(x)[0]):
        x1[i] = np.exp(x[i])/sum_raw[i]
    return x1

y_pred =softmax(logits)
CE1 = -np.sum(y*np.log(y_pred),-1)
print(CE1)
[1.68795487e-04 1.03475622e-03 6.58839038e-02 2.66698414e+00
 5.49852354e-02]
CE2 = sess.run(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y,logits=logits))

```