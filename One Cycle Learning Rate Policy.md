# A DISCIPLINED APPROACH TO NEURAL NETWORK HYPER-PARAMETERS: PART 1 – LEARNING RATE,BATCH SIZE, MOMENTUM, AND WEIGHT DECAY

## 摘要
尽管深度学习在过去纪念中已经在图像、语音、视频处理一个用中取得了眼花缭乱的成功，但大多数是用**次优**超参进行训练的,需要不必要的长时间训练时间。设置超参是一个黑盒需要多年的经验。本报告提出了几种有效设置超参的方法能有效的缩短训练时间提高性能。具体而言，本报告展示如何检查欠拟合和过拟合的训练集和测试集的损失函数，并提出走向最佳平衡点的指导方针。然后讨论如何增加、减小学习率和动量加速训练。我们的实验表明，平衡任何数据集和框架的各种正则化方式是很重要的。权重衰减（weight decay）用作一个样本正则以展示它的优化值是如何与学习率和动量紧密联系的。帮助复现此结果的报告文件在这里https://github.com/lnsmith54/hyperParam1.

## 介绍
深度学习取得了巨大的成功，但是深度学习的应用依然是一门黑盒艺术，往往需要多年的经验才能有效的选择超参、正则和模型框架。目前设置超参、设计网络框架的过程，需要专业的知识，广泛的实验和试错。它更多是基于意外的发现而非科学。另一方便，让深度学习尽可能的简单是一个公认的需求。

当前没有一个简单的方法取设置超参(learning rate，batchszie，momentum，weight decay).
网格搜索、随机搜索计算大耗时。然而，训练时间和最终性能高度依赖于好的选则。另外，从业者常从github、深度学习框架的model zoo 里选择标准的模型和超参，到那时这些参数不一定适合你的数据。

