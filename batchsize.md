

## batch size 是越大越好吗？
1. Bk=1,即随机梯度下降，即每次针对单个数据进行参数更新。这种方法的优势在于占用的内存很小，能够实现在线学习。弊端也很明显，就是很容易由于梯度方向的随机而导致模型无法收敛，而且无法充分并行计算，训练时间过长。

2. Bk=M，即每次都使用全部数据来进行模型参数的更新，这种方式在每次更新时，得到的更新梯度方向更加准确，使得模型收敛的更加稳定。但是缺点在于，当数据量非常大的时候，由于内存的限制，往往无法实现，同时所需要训练的epoch数也会大大增加；

3. 1<Bk<M,即小批量的梯度下降，这种方法权衡了随机梯度下降和全批量梯度下降的优缺点，往往在实际使用中能取得更好的效果。

另外根据2017年的一篇论文，过大的batch往往会降低模型的性能和泛化能力，那是因为多大的batch会使得在模型陷入到尖锐的局部最优处（Sharp Minimum）时，无法跳出来。而小一点的batch则可以由于梯度方向的变化大，而容易收敛一个平坦的最优处（Flat Minimum）。而往往在平坦的最优处能够在测试集上获得更好的泛化效果。