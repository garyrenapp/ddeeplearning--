## 权重正则化
![](imgs/zz-1.jpg)
* 吴恩达课上的解释: 要损失函数J最小化，则$\frac{\lambda}{2m}\sum{\left|W\right\|_2}$ 需要最小化，其实就是w-weight最小化，权重小则意味着影响小，我们把$\lambda$设置的足够大的话极端的情况就是w约等于0，网络退化到线性模型。
  

## L1 和 L2 的区别


## drop out 正则化

### 训练和测试
```python
d3 = np.random.rand(a3.shape[0],a3.shape[1]) < keep_prob
a3 = np.multiply(a3,d3)
a3/ = keep_prob  #训练阶段 除以keep_prob

#测试阶段无dropout，也不除以keep_prob
```

## 数据增广 
## early stopping
